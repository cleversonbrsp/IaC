Voc√™ deve ter baixado e instalado a OCI CLI vers√£o 2.24.0 (ou superior) e configurado corretamente. Se a sua vers√£o da OCI CLI for anterior √† 2.24.0, fa√ßa o download e instale uma vers√£o mais recente. Caso n√£o tenha certeza da vers√£o instalada, verifique com o seguinte comando:  

```sh
oci -v
```

### 1. Criar diret√≥rio para o kubeconfig  

Para armazenar o arquivo de configura√ß√£o do kubeconfig, crie o diret√≥rio necess√°rio:  

```sh
mkdir -p $HOME/.kube
```

### 2. Gerar o kubeconfig do cluster  

- **Para acesso via endpoint p√∫blico:**  

```sh
oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.sa-saopaulo-1.aaaaaaaauhpbdbxqgvl7dbokt33p6otriep24ugd5o62xi42bcbblbjy7isa \
  --file $HOME/.kube/config \
  --region sa-saopaulo-1 \
  --token-version 2.0.0 \
  --kube-endpoint PUBLIC_ENDPOINT
```

- **Para acesso via endpoint privado:**  

```sh
oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.sa-saopaulo-1.aaaaaaaauhpbdbxqgvl7dbokt33p6otriep24ugd5o62xi42bcbblbjy7isa \
  --file $HOME/.kube/config \
  --region sa-saopaulo-1 \
  --token-version 2.0.0 \
  --kube-endpoint PRIVATE_ENDPOINT
```

### 3. Definir a vari√°vel de ambiente KUBECONFIG  

Para garantir que o kubeconfig seja utilizado corretamente pelo `kubectl`, defina a vari√°vel de ambiente:  

```sh
export KUBECONFIG=$HOME/.kube/config
```

Caso queira salvar o kubeconfig em um local diferente, modifique o argumento `--file` no comando da CLI acima. Se necess√°rio, atualize o seu script de inicializa√ß√£o do shell para persistir a vari√°vel `KUBECONFIG`.

---
### **Sobre o provisionamento do Dynamic Group

O OCID do compartimento do OKE (`oci_identity_compartment.oke_comp.id`) s√≥ est√° dispon√≠vel depois que ele √© provisionado, precisamos ajustar a **Dynamic Group** para contornar essa limita√ß√£o.  

Infelizmente, o **`matching_rule` exige um valor est√°tico**, ent√£o **n√£o pode depender de recursos do Terraform**. Mas h√° uma alternativa:  
- Voc√™ pode **executar o Terraform em duas etapas**:  
  1. **Criar o compartimento (`oci_identity_compartment.oke_comp`)** e obter seu OCID.  
  2. **Executar um segundo `terraform apply`**, agora que o OCID j√° est√° dispon√≠vel, para criar a Dynamic Group corretamente.  

---

### **Op√ß√£o 1: Duas Execu√ß√µes (`terraform apply` em etapas)**
1. Execute o Terraform **apenas at√© provisionar o compartimento**:
   ```sh
   terraform apply -target=oci_identity_compartment.oke_comp
   ```
2. Copie o OCID gerado e adicione manualmente no `matching_rule` do **Dynamic Group**.
3. Execute o Terraform novamente para criar a Dynamic Group e a Policy:
   ```sh
   terraform apply
   ```

**Vantagem:** Funciona sem precisar modificar o c√≥digo.  
**Desvantagem:** Exige uma execu√ß√£o manual extra.

---

### **Op√ß√£o 2: Definir a Dynamic Group Manualmente**
Outra abordagem seria criar o **Dynamic Group diretamente pelo console da OCI**, usando o OCID do compartimento ap√≥s provision√°-lo. Assim, o Terraform n√£o precisaria gerenci√°-lo.

**Vantagem:** Evita complica√ß√µes no Terraform.  
**Desvantagem:** Exige um passo manual na configura√ß√£o inicial.

---

### **Op√ß√£o 3: Usar um Script para Automatizar**
Se quiser manter tudo automatizado, voc√™ pode rodar um **script Terraform + OCI CLI** para:
1. Criar o compartimento (`terraform apply -target=oci_identity_compartment.oke_comp`).
2. Obter o OCID via **OCI CLI**:
   ```sh
   oci iam compartment get --compartment-id <compartment_ocid>
   ```
3. Atualizar dinamicamente o Terraform para incluir o OCID no `matching_rule`.
4. Rodar `terraform apply` novamente.

Isso poderia ser feito via um script Bash, PowerShell ou mesmo um m√≥dulo no Terraform.  

---

### **Conclus√£o**
Se o objetivo √© **manter tudo no Terraform**, ent√£o **a melhor alternativa √© executar o `terraform apply` em duas etapas**. N√£o h√° como referenciar diretamente `oci_identity_compartment.oke_comp.id` no `matching_rule`, pois a OCI exige um valor fixo.  

Se quiser algo mais automatizado, um **script com a OCI CLI** pode resolver isso. üöÄ

---

### **Testar o Cluster Autoscaler**

Para testar o **Cluster Autoscaler** no **Oracle Kubernetes Engine (OKE)**, voc√™ pode simular uma carga alta no cluster de algumas maneiras, for√ßando a cria√ß√£o de novos n√≥s quando os recursos forem insuficientes. Aqui est√£o os principais m√©todos:

---

### üöÄ **1. Criando PODs que Consomem Muitos Recursos**
A maneira mais direta de testar o autoscaler √© criar **Pods que consomem muita CPU e mem√≥ria**, for√ßando o OKE a escalar novos n√≥s.

üìå **Crie um deployment com muitos recursos**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stress-test
spec:
  replicas: 5  # Aumente para estressar mais o cluster
  selector:
    matchLabels:
      app: stress
  template:
    metadata:
      labels:
        app: stress
    spec:
      containers:
      - name: stress-ng
        image: polinux/stress
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
        command: ["stress"]
        args: ["--cpu", "2", "--vm", "2", "--vm-bytes", "512M", "--timeout", "300s"]
```
üìå **O que esse deployment faz?**
- Ele cria 5 r√©plicas de um container usando a imagem `polinux/stress`
- Cada POD vai usar **2 vCPUs** e **512MB de mem√≥ria**, for√ßando o cluster a escalar novos n√≥s quando os recursos atuais forem insuficientes.

üîπ **Aumente os valores** para consumir mais recursos e testar a escalabilidade do cluster.

---

### üîÑ **2. Usando o HPA (Horizontal Pod Autoscaler)**
Se o **HPA** estiver habilitado no seu cluster, voc√™ pode configurar um autoscaler para aumentar o n√∫mero de r√©plicas automaticamente.

üìå **Crie um HPA para o Deployment acima**:
```sh
kubectl autoscale deployment stress-test --cpu-percent=50 --min=5 --max=20
```
Isso far√° com que o Kubernetes aumente as r√©plicas do **stress-test** quando o uso de CPU ultrapassar 50%.

---

### üî• **3. Gerando Carga com um Job de Teste**
Voc√™ pode usar **`kubectl run`** para criar um pod que executa um teste de carga no cluster.

```sh
kubectl run stress-cpu --image=busybox --restart=Never -- /bin/sh -c "yes > /dev/null &"
```
Esse comando cria um Pod que usa CPU indefinidamente, ajudando a testar o autoscaler.

---

### üìà **4. Monitorando o Escalonamento**
Ap√≥s rodar os testes, monitore se novos n√≥s est√£o sendo adicionados ao cluster:

üìå **Veja os n√≥s do cluster**:
```sh
kubectl get nodes
```
üìå **Acompanhe os pods e r√©plicas**:
```sh
kubectl get pods -o wide
kubectl get hpa
```
üìå **Acompanhe os eventos do cluster**:
```sh
kubectl get events --sort-by=.metadata.creationTimestamp
```

---

### üöÄ **Conclus√£o**
Com esses testes, voc√™ deve ver o **OKE escalando automaticamente os n√≥s** do cluster quando os pods come√ßarem a consumir mais CPU e mem√≥ria. Quando a carga diminuir, os n√≥s extras devem ser removidos.

Fontes:
https://docs.oracle.com/pt-br/iaas/Content/ContEng/Tasks/contengusingclusterautoscaler.htm
https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/oci/README.md
https://docs.oracle.com/en/cloud/paas/weblogic-container/user/create-dynamic-groups-and-policies.html
https://docs.oracle.com/en-us/iaas/Content/Identity/compartments/Working_with_Compartments.htm#Working

